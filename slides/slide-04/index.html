<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Slide 4 · Understanding LLMs</title>
    <link rel="stylesheet" href="style.css" />
  </head>
  <body data-prev="../slide-03/index.html" data-next="../slide-05/index.html">
    <main class="slide">
      <header>
        <p class="badge">Slide 4 · Technology Focus</p>
        <h1>Understanding the Technology</h1>
        <p class="lede">
          Large Language Models (LLMs) translate text prompts into visuals by
          learning patterns from immense datasets.
        </p>
      </header>

      <figure class="visual">
        <div class="image-suggestion">
          Image suggestion: Minimal infographic showing prompt → LLM →
          generated artwork pipeline.
        </div>
      </figure>

      <section class="content">
        <div class="accordion">
          <details class="point">
            <summary>
              <span class="label">What are LLMs?</span>
              <span class="snippet"
                >Pattern-learners trained on oceans of human-created text &amp;
                imagery.</span
              >
            </summary>
            <div class="detail-content">
              <p class="detail-text">
                They recognize linguistic and visual structures, then predict the
                next best token or pixel. No mystery—just statistics over data we
                authored.
              </p>
            </div>
          </details>

          <details class="point">
            <summary>
              <span class="label">Human-made training data</span>
              <span class="snippet"
                >Models only echo what people have documented.</span
              >
            </summary>
            <div class="detail-content">
              <p class="detail-text">
                If our datasets omit a culture, style, or story, the model cannot
                surface it. LLMs remix the knowledge we donate; they cannot
                conjure what humanity hasn’t yet recorded.
              </p>
            </div>
          </details>

          <details class="point">
            <summary>
              <span class="label">How they work in art</span>
              <span class="snippet"
                >Prompts are translated into latent visual instructions.</span
              >
            </summary>
            <div class="detail-content">
              <p class="detail-text">
                An LLM parses the prompt, associates it with learned visual
                cues, and guides a diffusion or transformer decoder to paint
                pixels, often iterating hundreds of steps per image.
              </p>
            </div>
          </details>

          <details class="point">
            <summary>
              <span class="label">Examples in visual arts</span>
              <span class="snippet"
                >Open and commercial systems span creation to editing.</span
              >
            </summary>
            <div class="detail-content">
              <ul class="sublist">
                <li>
                  <strong>Text-to-Image:</strong> DALL·E&nbsp;3, Midjourney,
                  Stable Diffusion.
                </li>
                <li>
                  <strong>Image Editing:</strong> Adobe Firefly, Photoshop
                  Generative Fill.
                </li>
              </ul>
            </div>
          </details>
        </div>
      </section>

    </main>

    <button class="nav-arrow nav-prev" type="button" aria-label="Previous slide">
      <span>&larr;</span>
    </button>
    <button class="nav-arrow nav-next" type="button" aria-label="Next slide">
      <span>&rarr;</span>
    </button>

    <script src="../shared.js"></script>
  </body>
</html>

